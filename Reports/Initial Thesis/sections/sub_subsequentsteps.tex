\documentclass[../initial_thesis.tex]{subfiles}

\begin{document}
\section{Introduction}
Let us briefly recall what has been covered thus far and how it relates to the rest of this project. From Chapter \ref{chap:background}, the information gained from the review of the field of audiovisual expression, music feature extraction, and video synthesizers, gives us a basis of knowledge of the various concepts and techniques that have been used in building video synthesizers. The framework and board reviews in Chapter \ref{chap:toolreview}, on the other hand, helped to decide the tools that will be used in the implementation of this project. This chapter will cover the design of the audiovisualisers and implementing them on the Odroid XU4 --- thereby forming the video synthesizer. \par

There are several steps that are taken in designing the audiovisuals. The first task would be to design a visual language: a mapping of forms of visual movement to various low-level audio features, such as those listed in Section \ref{sec:lowlevelaudio}, for the video synthesizer. The design of this visual language will be guided by the works of Michel Chion, Adriana Sa, as well as some of the current implementations highlighted in the framework review. Once this has been decided, work will begin in implementing this visual language using openFrameworks --- the chosen framework from Section \ref{sec:frameworkreview}. The visuals will respond to the parameters of a live audio input, thereby forming an audiovisualisation. This will be followed by implementing the video synthesizer on the Odroid XU4, as decided upon in Section \ref{sec:boardreview}. Having done so, the computation timings of the audiovisualisations on the SBC will be measured. User testing will then be used to shape iterative prototypes of the visual language. The following sections cover these steps, thus showing how the video synthesizer was implemented. 

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../initial_thesis"
%%% End: