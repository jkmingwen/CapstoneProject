\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{listings} % this is to allow code formatting in document
\lstset
{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}
\usepackage[labelformat=simple]{subcaption}
\renewcommand\thesubfigure{(\alph{subfigure})}
\usepackage{graphicx} % allow for figures
\usepackage{url} % make url hyperlinks work
\graphicspath{ {graphics/} } % set graphics path
\usepackage{subfiles} % allow for subfiles
\usepackage[table,xcdraw]{xcolor} % allow for table creation
% make paths for sub-directories
\makeatletter
\providecommand*{\input@path}{}
\g@addto@macro\input@path{{./sections//}{./sections/subsections}{./subsections}}
\makeatother
\usepackage{biblatex} % use biblatex for citations
\addbibresource{../../../BibTeXFiles/thesis_ref.bib} % set bib file for citations
\usepackage{hyperref} % better linking in document
\usepackage{pdfpages} % allow for multipage pdf (must be placed AFTER 'xcolor' package for some reason)
\usepackage[toc, page]{appendix} % package to add appendix environment
\usepackage[section]{placeins} % makes sure that figures stay within the sections they're declared in

\title{From Audio to Video Signals in Real-time: Design and Implementation of a Portable Video Synthesizer (Initial Thesis)}
\author{Jaime Koh}

\begin{document}
\includepdf[pages=-]{Cover/coverpage.pdf}
% \includepdf[pages=-]{Cover/declaration.pdf}
\chapter*{Acknowledgements}
\markright{}
\addcontentsline{toc}{section}{Acknowledgements}

\begin{enumerate}
\item Bruno Bodin
\item Jon He
\item Nikki Chen
\item Suitemates
\item Family
\item Friends
\item MCS Faculty \& Peers
\end{enumerate}

\begin{abstract}
  My paper is about music and animations. 
\end{abstract}

\newpage
\tableofcontents
\newpage

\chapter{Introduction}
% check citations for this paragraph!
Video synthesizers started off as ``machines that electronically manipulated a video signal or a cathode ray tube to produce abstract or distorted images'' \cite{Collopy2014}. The various video synthesizers built by John and James Whitney \cite{Patterson2009}, Eric Siegel \cite{ElectronicArtsIntermix}, Nam June Paik and Shuya Abe \cite{Furlong1983}, and Stephen Beck \cite{Beck1992} --- amongst others --- expanded the scope of methods with which visuals could be produced. The gradual digitalisation of video synthesizers introduced even more means with which one could manipulate and produce a visual signal \cite{Collopy2014}. A video synthesizer can thus be broadly understood as a tool that takes in some form of input and manipulates or utilises said input to produce a visual signal. One possible input for a video synthesizer is an audio signal --- this is not an uncommon technique --- both Windows Media Player and iTunes have music visualisation functions that react to the audio signal of the media being played. These add a visual dimension to what was once purely an aural sensory experience and allows users to enjoy music in more active ways than conventional passive music consumption \cite{Casey2008}. \par

There are several video synthesizers made specifically for music visualisation that are designed to run off a computer \cite{Casey2008}, however, such an implementation makes them cumbersome for use in a live context --- space on a stage can be sparse, and having to bring a computer to performances is inconvenient, especially if that means having to set up another piece of equipment. The solution to this is to have a video synthesizer that is implemented in a more portable enclosure. These do exist, nonetheless, most portable video synthesizers offer only simple graphics, such as those seen in Figure \ref{fig:etc}.

\begin{figure}[b]
  \begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width = 0.9\linewidth]{Report2/etc_mode}
    \caption{Screenshot \textit{ETC} video output}
    \label{fig:etc_mode}
  \end{subfigure}
  \begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width = 0.9\linewidth]{Report2/etc_modeimages}
    \caption{\textit{ETC} output using images}
    \label{fig:etc_modeimages}
  \end{subfigure}
  \caption{Visuals from the \textit{ETC}}
  \label{fig:etc}
\end{figure}

\paragraph{A proposed solution}
This project addresses the shortcomings of the current range of video synthesizers by taking advantage of the increasingly powerful processing capabilities of single-board computers (SBCs). Its aim is to design and implement a portable video synthesizer that takes in an audio signal as input and --- using information extracted from the audio signal --- produces and manipulates a visual output in real-time. For the purpose of the paper, real-time is defined according to Wessel and Wright's definition of control intimacy in their paper, \textit{Problems and Prospects for Intimate Musical Control of Computers}: 10 milliseconds (ms) with a variation not exceeding 1 ms \cite{Wessel2002}. The goal is to design and implement a video synthesizer that will (1) produce complex and aesthetically pleasing visuals that react in real-time to a musician's playing and (2) integrate smoothly into a musician's live setup. \par

This thesis is split into three chapters: (1) Background, (2) Reviewing Tools of Implementation, and (3) Audiovisual Design. Chapter \ref{chap:background} covers the motivations and research done on combining visuals with audio, and how music information can be extracted from an audio signal. Readers are orientated to the field of audiovisual expression, the relevant techniques used in Music Information Retrieval (MIR), as well as some of the portable video synthesizers built for music visualisation currently available on the market. Brief definitions of the terminologies within these fields have also been included when relevant, along with references to more in depth readings pertaining to the given field. The research done in these fields will shape the ways in which the project is implemented. \par

Chapter \ref{chap:toolreview} consists of research on frameworks and SBCs --- it illustrates how the software and hardware tools used in implementing this project have been be selected. Five frameworks and six SBCs were considered altogether. Each framework was reviewed based on its learning curve, popularity, and existing projects utilising it, with openFrameworks ultimately being selected. The SBCs were evaluated based on their processing capablities as well as their hardware capablities. From this review, the Odroid XU4 was chosen as the SBC most suitable for this project. openFrameworks and the Odroid XU4 therefore form the tools with which this project will be implemented. \par

The process of implementing the video synthesizer is covered in Chapter \ref{chap:avdesign}. The initial animations implemented are explained along with the design and results of a survey that collected feedback on these audiovisualisations. A final series of animations were designed based off the feedback received from the surveys before they were implemented on the Odroid XU4. The chapter ends with a performance review of the video synthesizer on the Odroid XU4. \par

The paper ends with a summary of what has been accomplished over the course of this project. It also includes thoughts about how the work done might contribute to the field of audiovisualisation, with a specific focus on how we might approach designing improved video synthesizers.

% Control intimacy: Can introduce later (just say latency for now) --- think about Chion and how we want to link images tgt and this can help with avoiding perceptions of latency
% It's an improvement cause there's intent to evoke emotional responses from audiences and not just present information

\chapter{Background}\label{chap:background}
\subfile{sections/sub_audiovisual}
\subfile{sections/sub_mir}
\subfile{sections/sub_videosynthesizers}

\chapter{Reviewing Tools of Implementation}\label{chap:toolreview}
\subfile{sections/sub_frameworkreview}
\subfile{sections/sub_boardreview}

\chapter{Audiovisual Design}\label{chap:avdesign}
\subfile{sections/sub_subsequentsteps}
\subfile{sections/sub_animations}
\subfile{sections/sub_timings}
\subfile{sections/sub_surveys}
\subfile{sections/sub_futureimplementations}

\chapter{Conclusion}
There are several things that were accomplished in this study. By reviewing the fields of audiovisual expression and existing video synthesizers, we established the motivations behind implementing a video synthesizer that maps audio inputs to visual outputs. The field of MIR provided us with the appropriate means of extracting and measuring low-level audio features. Tracking these low-level audio features allows us to map features of an audio signal to changes in the visuals produced. The review of frameworks and boards gave us the tools which were used to implement a video synthesizer.

Ultimately, this project showed that a video synthesizer that maps a live audio signal to complex visuals in real-time is achievable using current open-source frameworks --- furthermore, it showed that these audiovisuals can be implemented on an affordable SBC and remain within the bounds of just-noticable latency. Implementing a portable video synthesizer that translates audio to visual signals in real-time is therefore made feasible by utilising the processing capabilities of increasingly powerful, compact, and affordable SBCs available to us. Moving forward, in designing visuals that react to an audio signal, it would be interesting to observe the ability for different audiovisualisations to hold a viewer's attention and if any patterns emerge from a larger scale study similar to the survey conducted. It would also be interesting to see how complex the visuals can be while continuing to react to an audio signal in real-time.
\printbibliography

% Appendices section
\begin{appendices}
  \subfile{sections/sub_appendix}
\end{appendices}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
